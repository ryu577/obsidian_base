
#todo #links 
https://lilianweng.github.io/posts/2020-01-29-curriculum-rl/

$X\beta = y$

# Least squares
Taken from: https://stats.stackexchange.com/questions/46151/how-to-derive-the-least-square-estimator-for-multiple-linear-regression

Starting from $y= Xb +\epsilon$, which really is just the same as

$$\begin{bmatrix}
y_{1} \\
y_{2} \\
\vdots \\
y_{N}
\end{bmatrix}
=
\begin{bmatrix}
x_{11} & x_{12} & \cdots & x_{1K} \\
x_{21} & x_{22} & \cdots & x_{2K} \\
\vdots & \ddots & \ddots & \vdots \\
x_{N1} & x_{N2} & \cdots & x_{NK}
\end{bmatrix}
.
\begin{bmatrix}
b_{1} \\
b_{2} \\
\vdots \\
b_{K}
\end{bmatrix}
+
\begin{bmatrix}
\epsilon_{1} \\
\epsilon_{2} \\
\vdots \\
\epsilon_{N}
\end{bmatrix} $$

$$\begin{bmatrix}
y_{1} \\
y_{2} \\
\vdots \\
y_{n}
\end{bmatrix}
=
\begin{bmatrix}
1 & x_{1,1} & x_{1,2} & \cdots & x_{1,m-1} \\
1 & x_{2,1} & x_{2,2} & \cdots & x_{2,m-1} \\
\vdots & \ddots & \ddots & \vdots \\
1 & x_{n,1} & x_{n,2} & \cdots & x_{n,m-1}
\end{bmatrix}_{n\times m}
.
\begin{bmatrix}
b_{1} \\
b_{2} \\
\vdots \\
b_{m}
\end{bmatrix}
+
\begin{bmatrix}
\epsilon_{1} \\
\epsilon_{2} \\
\vdots \\
\epsilon_{n}
\end{bmatrix} $$
$$
\begin{bmatrix}
1 & x_{1,1} & x_{1,2} & \cdots & x_{1,m-1} \\
1 & x_{2,1} & x_{2,2} & \cdots & x_{2,m-1} \\
\vdots & \ddots & \ddots & \vdots \\
1 & x_{n,1} & x_{n,2} & \cdots & x_{n,m-1}
\end{bmatrix}_{n\times m}
.
\begin{bmatrix}
\beta_{0} \\
\beta_{1} \\
\vdots \\
\beta_{m-1} 
\end{bmatrix} = \begin{bmatrix}
y_{1} \\
y_{2} \\
\vdots \\
y_{n}
\end{bmatrix}$$


It all comes down to minimizing $e'e$:

$$\epsilon'\epsilon = \begin{bmatrix}
e_{1} & e_{2} & \cdots & e_{N} \\
\end{bmatrix}
\begin{bmatrix}
e_{1} \\
e_{2} \\
\vdots \\
e_{N}
\end{bmatrix} = \sum_{i=1}^{N}e_{i}^{2}$$

So minimizing $e'e$ gives us:

$\min_{b}$ $e'e = (y-Xb)'(y-Xb)$

$\min_{b}$ $e'e = y'y - 2b'X'y + b'X'Xb$

$\frac{\partial(e'e)}{\partial b} = -2X'y + 2X'Xb = 0$

$X'Xb=X'y$

$b=(X'X)^{-1}X'y$

# Column space


$$|\vec{d}|^2 = (X\vec{\beta}-\vec{y})^T.(X\vec{\beta}-\vec{y})$$
$$|\vec{d}|^2=\vec{\beta}^TX^TX\vec{\beta}-\vec{y}^TX\vec{\beta}-\vec{\beta}^TX\vec{y}+\vec{y}^T\vec{y}$$
$$\frac{\partial |\vec{d}|^2}{\partial\vec{\beta}} = 2X^TX\vec{\beta}-2X^T \vec{y}$$
$$\begin{array}22X^TX\vec{\beta}-2X^T \vec{y} = 0\\
=>\vec{\beta} = (X^TX)^{-1}(X^Ty)
\end{array}$$

$$\frac{\partial |\vec{d}|^2}{\partial\vec{\beta}} = \frac{\partial((X\vec{\beta}-\vec{y})^T.(X\vec{\beta}-\vec{y}))}{\partial\vec{\beta}} = 0$$
