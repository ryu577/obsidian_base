[[Diffusion]], [[DiffusionModels|blog]]
#cs/ai/diffusion 
# Main papers
- [Original stable diffusion](https://arxiv.org/pdf/2112.10752.pdf) [[stable_diffusion0.pdf|local]] High-Resolution Image Synthesis with Latent Diffusion Models.
	- [Diffusion models beat GANs](https://arxiv.org/pdf/2105.05233.pdf) appendix B has best explanation of what's going on.
- [Denoising Diffusion Probabilistic Models](https://arxiv.org/pdf/2006.11239.pdf) (Dec 2020) [[denoising_diffusion.pdf|local]] has the training algorithm.
- [Seminal paper](https://arxiv.org/pdf/1503.03585.pdf) 2015, [[theormodynamics_diffusion.pdf|local]] "Deep Unsupervised Learning using Nonequilibrium Thermodynamics"
- [Lilian Wang blog](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/).

## Other papers
- [SCORE-BASED GENERATIVE MODELING THROUGH STOCHASTIC DIFFERENTIAL EQUATIONS](https://arxiv.org/pdf/2011.13456.pdf)
- [Diffusion in physics and probability](https://scholar.harvard.edu/files/schwartz/files/2-diffusion.pdf).

# Original stable diffusion paper
https://arxiv.org/pdf/2112.10752.pdf
Code here: https://github.com/CompVis/stable-diffusion

The generative power of these models stems from a natural fit to the inductive biases of image-like data when their underlying neural back- bone is implemented as a UNet.

Similar to other types of generative models [56, 83], diffusion models are in principle capable of modeling conditional distributions of the form $p(z|y)$. This can be implemented with a conditional denoising autoencoder $\epsilon_\theta(z_t, t, y)$ and paves the way to controlling the synthesis process through inputs y such as text [68].

[15] Diffusion models beat GANs
## Diffusion models beat GANs
[Diffusion models beat GANs](https://arxiv.org/pdf/2105.05233.pdf)
$$q(x_t|x_{t-1}) = N(\sqrt{1-\beta_t}x_{t-1}, \beta_t I)$$

To train these models, each sample in a minibatch is produced by randomly drawing a data sample $x_0$, a timestep $t$, and noise $\epsilon$, which together give rise to a noised sample $x_t$.

(Source: [Denoising diffusion prob mdls](https://arxiv.org/pdf/2006.11239.pdf))
![[Screenshot 2023-12-30 at 1.58.56 PM.png#invert|300]]

![[Screenshot 2023-12-30 at 2.06.04 PM.png#invert|300]]
The UNet model uses a stack of residual layers and downsampling convolutions, followed by a stack of residual layers with upsampling convolutions, with skip connections connecting the layers with the same spatial size.
- [ ] What is the architecture of this UNet? Can I visualize it?
https://nn.labml.ai/diffusion/ddpm/unet.html

UNet architecture: https://towardsdatascience.com/you-cant-spell-diffusion-without-u-60635f569579
https://www.youtube.com/watch?v=NhdzGfB1q74&t=306s
https://nn.labml.ai/diffusion/ddpm/unet.html

[SCORE-BASED GENERATIVE MODELING THROUGH STOCHASTIC DIFFERENTIAL EQUATIONS](https://arxiv.org/pdf/2011.13456.pdf)

# Karpathy transformers
https://www.youtube.com/@AndrejKarpathy/videos

# GAN
The idea behind this method is to train a Discriminator (D) to correctly distinguish real samples from fake samples generated by a second agent, known as the Generator (G). GANs excel at generating high-quality samples as the discriminator captures features that make an image plausible, while the generator learns to emulate them.


# Row rank col rank blog
![[Screenshot 2023-12-30 at 2.11.04 PM.png#invert|380]]

- [[attention_all_u_need.pdf|attention paper]]
