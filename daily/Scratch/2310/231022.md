# 1. Terminology
- $n$: The number of VMs databricks requires to serve a single request.
- $q$: The SLA databricks wants to provide its customers.
- $\lambda$: The rate at which databricks customer requests come in. We assume one cache serves 1000 requests per day.
- $\mu$: The rate at which our cache is rehydrated (time we take to provision $n$ VMs).
- $m$: The number of VMs we hold in our cache.
- o: The amount by which we overprovision when rehydrating the cache.
- $l$: The average length of time a databricks job runs for. We assume 15 mins.
Based on these, we define a few more:
- $k=m/n$ is how much larger the cache is than the standard VM request size. It is an integer.
# 2. Assumptions
- Databricks requires $n$ VMs to serve any of their requests.
- We assume that if a cache miss happens, the SLA isn't achieved.
- We don't try to predict the future and modify the cache size dynamically. We keep the cache size constant at $m$ throughout the period we're modeling.
# 3. Approach
The assumptions imply that the cache size, $m$ should always be an integer multiple of $n$ (with $k$ being that multiple).

The discount, $d$ becomes (see section 5):
$$d = \frac{k}{k+\lambda l}$$

Now its only a matter of how to get the $k$. If we can make our $\mu$ (or average time to provision $n$ VMs for the cache) better (in relation to $\lambda$), we can get away with a smaller and smaller $k$. The rate at which the cache is used up by customers and the rate at which we can rehydrate it are two opposing forces. The graph below plots the probability of a cache miss on the y-axis with the rate at which we rehydrate the cache on the x-axis.
![[Screenshot 2023-10-22 at 5.13.37 PM.png#invert|400]]
This implies certain intervals where our average cache rehydration time can be for any given value of $k$.
![[Screenshot 2023-10-22 at 5.05.04 PM.png#invert|400]]

To get the $k$ we should pick to meet the SLA of $q$, we need to model the probabilistic process. It turns out to be equivalent to a queueing process. If the cache factor is $k$, its a queue with $k$ servers where customers are not allowed to stand in line (if they arrive when all $k$ servers are busy, they leave). The equivalent of a customer finding all $k$ servers busy in our case will be a cache miss. Thankfully, queueing systems are covered extensively in chapter 8 of [1](https://www.amazon.com/Introduction-Probability-Models-Sheldon-Ross/dp/0124079482). We use the techniques there (continuous time Markov chains) to model the process and obtain the values of $k$ required to meet the SLA, $q$.
# 4. Results
Let's assume $\lambda = 100/144=0.7$ requests per minute (1000 requests per day).

- At $k=1$ and $q=.001$, we get $\mu = 650$ per min. So avg cache recover time .1 sec.
	- Discount would be $\frac{1}{1+\lambda l} = 8.75\%$ 
- At $k=2$ and $q=.001$, we get $\mu=15$ per min. So avg cache recover time: 4 sec.
	- Discount would be $\frac{2}{2+\lambda l} = 16.1\%$
- At $k=3$ and $q=.001$, we get $\mu = 3.5$ per min. So avg cache recover time: 17 sec.
	- Discount would be $\frac{3}{3+\lambda l} = 22\%$
- At $k=4$ and $q=.001$, we get $\mu = 1.5$ per min. So avg cache recover time: 40 sec.
	- Discount would be $\frac{4}{4+\lambda l} = 27.7\%$
- At $k=5$ and $q = .001$, we get $\mu=.9$ per min. So avg cache recover time: 1.1 min.
	- Discount would be $\frac{5}{5+\lambda l} = 32\%$

Over-provisioning for the cache is an option and the more we over-provision, the lower our avg cache recover time becomes.
- Based on Claudia's data, the avg VM creation time is 16 sec.
- Based on [this](https://math.stackexchange.com/questions/80475/order-statistics-of-i-i-d-exponentially-distributed-sample), if we assume that the VM creation time is exponential,
	- The min of $n$ has avg time $\frac{16}{n}$
	- The second smallest of $n$ has avg time $16(\frac{1}{n}+\frac{1}{n-1})$
	- And so on.
- So, if we overprovision to 80 VMs, the avg time to get $n=40$ VMs will be 11.2 sec.
	- If we overprovision to 200 VMs, the avg time to get $n=40$ will be <4 sec. 
		- This would put us in $k=2$ world.
- This will put us in $k=3$. The discount is 22%.
	- But that discount is if we intend to cover the entire cost of the cache.
	- Instead, we want to track towards North star. Let's say North star is $k=2$.
	- Then, the discount should be $0$ at $k=2$ and so we need to shift the entire curve downwards. This leads to the discount at $k=3$: 6%.
	- If we don't overprovision, the time to get $n=40$ VMs becomes 64 sec and we end up all the way in $k=5$. Then, the discount becomes 32-16 = 16%.

# 5. Details
Derivation of the discount factor:
![[Screenshot 2023-10-22 at 4.33.56 PM.png#invert|400]]


For the special case of an M/M/k queue, we get a closed form for the probability of a cache miss. It is $P_k$ in the derivation below.
![[Screenshot 2023-10-22 at 4.35.58 PM.png#invert|400]]
Implemented below.
```python
def state_p(lmb, mu, k):
	term = 1
	summ = 0
	for kk in range(k):
		summ += term
		term = term*lmb/(kk+1)/mu
	return term/(summ+term)
```

A simulator that models an M/M/k queue where the queue size is zero.
```python
import heapq
import numpy as np
lmb = 3
mu = 10
k = 2
t1 = 0
t2 = 0
req = 0
cache_miss = 0
arr = []
heapq.heapify(arr)
while t1 < 50000:
	req += 1
	if len(arr) < k:
		t2 = t1+np.random.exponential(1/mu)
		heapq.heappush(arr, t2)
	t1 = t1+np.random.exponential(1/lmb)
	if len(arr) == k:
		if arr[0] > t1:
			cache_miss += 1
	else:
		while len(arr) > 0 and arr[0] < t1:
			heapq.heappop(arr)

print(cache_miss/req)
```
#math/prob/markov #math/prob/exponential keywords: simulation simuln #math/prob/poisson 2310 queue continuous time markov m/m/1 m/m/k

The closed form and simulation agree with each other for the case of an M/M/k queue.
# 6. References
[1](https://www.amazon.com/Introduction-Probability-Models-Sheldon-Ross/dp/0124079482) Introduction to probability models by Sheldon Ross (10th edition)